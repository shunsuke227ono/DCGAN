{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN([Radford et al. (2015)](https://arxiv.org/abs/1511.06434)解説)\n",
    "\n",
    "## GANの基本\n",
    "\n",
    "GANでは, GeneratorとDiscrminatorという2つのネットワークが登場します。Generatorは訓練データと同じようなデータを生成しようとします。一方、discriminatorはデータが訓練データから来たものか、それとも生成モデルから来たものかを識別します。\n",
    "\n",
    "<img src=\"https://elix-tech.github.io/images/2017/gan/gan.png\">\n",
    "\n",
    "## DCGAN\n",
    "\n",
    "今回実装するのは, [Radford et al. (2015)](https://arxiv.org/abs/1511.06434)で提案されたDCGAN(Deep Convolutional GAN)です。 DCGANは, 下図のように, CNN(convolutional neural network)を使用したモデルです。 画像生成するには, GANでもCNNを使うのが良いと考えたものです。\n",
    "\n",
    "<img src=\"https://elix-tech.github.io/images/2017/gan/dcgan_generator.png\">\n",
    "\n",
    "後述する、学習をうまく進めるためのテクニックも紹介されています。\n",
    "\n",
    "### プーリングをやめる\n",
    "\n",
    "* Discriminator: CNNでは最大プーリングを用いてダウンサンプリングするのが一般的ですが、DCGANのdiscriminatorでは, stride2の畳み込みで行います。\n",
    "* Generator: transposed convolutionを使ってアップサンプリングを行います。\n",
    "\n",
    "### 全結合層でなく, global average poolingを使う\n",
    "\n",
    "CNNでは最後の方の層で全結合層になっていることがよくありますが、DCGANのdiscrimatorでは全結合層ではなくglobal average poolingを用います。(Global average pooling 参考: [Lin et al. (2013)](https://arxiv.org/abs/1312.4400))\n",
    "\n",
    "全結合層では通常ドロップアウトを使って過学習を防ぐ必要がありますが、global average poolingではパラメータがなくなるため、過学習を防ぐ効果があります。\n",
    "\n",
    "### Batch Normalizationを使う\n",
    "\n",
    "DCGANではbatch normalizationをgeneratorとdiscriminatorの両方に適用します。但し、全ての層に適用すると不安定になってしまうようで、generatorの出力層と、discriminatorの入力層には適用しないようにします。\n",
    "\n",
    "### Leaky ReLuを使う\n",
    "\n",
    "Generatorでは活性化関数に出力層だけTanhを使いますが、それ以外の層では全てReLUを使います。一方、discriminatorの方では全ての層でLeaky ReLUを使います。\n",
    "\n",
    "### 実験結果\n",
    "\n",
    "[Radford et al. (2015)](https://arxiv.org/abs/1511.06434)における実験結果.\n",
    "\n",
    "ベッドルームの画像のデータ・セットを用いて学習した結果です。\n",
    "\n",
    "<img src=\"https://elix-tech.github.io/images/2017/gan/bedroom.png\">\n",
    "\n",
    "また, GANでも入力のzベクトルを使って, 演算を行うことができる結果も紹介されています。\n",
    "\n",
    "<img src=\"https://elix-tech.github.io/images/2017/gan/glasses.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 実装・実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "dir_name = 'ramen_image'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. データの用意\n",
    "\n",
    "任意のディレクトリに学習に使用するイメージを用意する. 以下の`dir_name`にディレクトリのパスを入れる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_name = './source_imgs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discrminatorの定義\n",
    "\n",
    "* 最大プーリングではなく, stride2で畳み込みを行う\n",
    "* LeakyReluの使用\n",
    "* 入力層以外にBatch Normalizationを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        convLayers = [\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        self.convLayers = nn.Sequential(*convLayers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        X = self.convLayers(input)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generatorの定義\n",
    "\n",
    "* 出力層以外でBatchNormalizationを用いる.\n",
    "* transposed convolutionを使用したアップサンプリングを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        tconvLayers = [\n",
    "            nn.ConvTranspose2d(100, 256, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        self.tconvLayers = nn.Sequential(*tconvLayers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        X = self.tconvLayers(input)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず, 便利さのため画像を読み込む関数を用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_images():\n",
    "    files = os.listdir(dir_name)\n",
    "    result = []\n",
    "    for i in files:\n",
    "        img = cv2.imread('%s/%s' % (dir_name, i))\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "        img = img.transpose(2, 0, 1) / 255.\n",
    "        result.append(img)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = read_images()\n",
    "shuffle(images)\n",
    "\n",
    "batch_size = 10\n",
    "epoch = 10000\n",
    "image_size = images.shape[0]\n",
    "epoch_time = math.ceil(image_size / batch_size)\n",
    "\n",
    "D = Discriminator()\n",
    "G = Generator()\n",
    "optimizer_D = optim.Adam(D.parameters())\n",
    "optimizer_G = optim.Adam(G.parameters())\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss() # LSGan\n",
    "\n",
    "for e in range(epoch):\n",
    "    for i in range(epoch_time):\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_image = images[start:end]\n",
    "        minibatch_size = len(batch_image)\n",
    "        img_tensor = Variable(torch.FloatTensor(batch_image))\n",
    "        one_labels = Variable(torch.ones(minibatch_size, 1, 1, 1))\n",
    "        zero_labels = Variable(torch.zeros(minibatch_size, 1, 1, 1))\n",
    "        fixed_noise = torch.FloatTensor(minibatch_size, 100, 1, 1)\n",
    "        fixed_noise = Variable(fixed_noise.normal_(0, 1))\n",
    "        generated = G(fixed_noise)\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            real_output = D(img_tensor)\n",
    "            fake_output = D(generated.detach())\n",
    "\n",
    "            d_loss = criterion(real_output, one_labels) + criterion(fake_output, zero_labels)\n",
    "            if d_loss.data[0] > 0.1:\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                print(\"EPOCH :%s, D: %s\" % (e, d_loss.data[0]))\n",
    "        else:\n",
    "            fake_output = D(generated)\n",
    "            g_loss = criterion(fake_output, one_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            print(\"EPOCH :%s, G: %s\" % (e, g_loss.data[0]))\n",
    "    torch.save(D, \"D.model\")\n",
    "    torch.save(G, \"G.model\")\n",
    "    fixed_noise = torch.FloatTensor(1, 100, 1, 1)\n",
    "    fixed_noise = Variable(fixed_noise.normal_(0, 1))\n",
    "    generated = G(fixed_noise)\n",
    "    save_generated_image = generated[0].cpu().data.numpy().transpose(1, 2, 0) * 255.\n",
    "    scipgy.misc.imsave(\"generated/\"+str(e)+\".jpg\", save_generated_image.astype(np.uint8)[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Appendix\n",
    "\n",
    "* [Goodfellow et al. (2014)](https://arxiv.org/abs/1406.2661)\n",
    "* [Radford et al. (2015)](https://arxiv.org/abs/1511.06434)\n",
    "* [はじめてのGAN](https://elix-tech.github.io/ja/2017/02/06/gan.html)\n",
    "* [How to Train a GAN](https://github.com/soumith/ganhacks)\n",
    "* [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
